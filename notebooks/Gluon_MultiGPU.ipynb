{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.io import DataDesc\n",
    "from mxnet import nd, gluon, autograd\n",
    "from mxnet.gluon.data import RecordFileDataset, ArrayDataset, Dataset\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "from mxnet.gluon.data.vision.datasets import ImageFolderDataset\n",
    "from mxnet.gluon.data.dataloader import DataLoader\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from mxnet import recordio\n",
    "\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from common.utils import *\n",
    "from common.params_dense import *\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) \n",
      "[GCC 7.2.0]\n",
      "Numpy:  1.13.3\n",
      "MXNet:  1.2.0\n",
      "GPU:  ['Tesla V100-SXM2-16GB', 'Tesla V100-SXM2-16GB', 'Tesla V100-SXM2-16GB', 'Tesla V100-SXM2-16GB']\n",
      "CUDA Version 9.1.85\n",
      "CuDNN Version  7.1.3\n"
     ]
    }
   ],
   "source": [
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"MXNet: \", mx.__version__)\n",
    "print(\"GPU: \", get_gpu_name())\n",
    "print(get_cuda_version())\n",
    "print(\"CuDNN Version \", get_cudnn_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUs:  32\n",
      "GPUs:  4\n"
     ]
    }
   ],
   "source": [
    "# User-set\n",
    "# Note if NUM_GPUS > 1 then MULTI_GPU = True and ALL GPUs will be used\n",
    "# Set below to affect batch-size\n",
    "# E.g. 1 GPU = 64, 2 GPUs =64*2, 4 GPUs = 64*4\n",
    "# Note that the effective learning-rate will be decreased this way\n",
    "CPU_COUNT = multiprocessing.cpu_count()\n",
    "GPU_COUNT = len(get_gpu_name())\n",
    "MULTI_GPU = GPU_COUNT > 1\n",
    "print(\"CPUs: \", CPU_COUNT)\n",
    "print(\"GPUs: \", GPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually scale to multi-gpu\n",
    "if MULTI_GPU:\n",
    "    LR *= GPU_COUNT\n",
    "    BATCHSIZE *= (GPU_COUNT)\n",
    "    BATCHSIZE = BATCHSIZE//GPU_COUNT*GPU_COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model-params\n",
    "# Paths\n",
    "CSV_DEST = \"/data/chestxray\"\n",
    "IMAGE_FOLDER = os.path.join(CSV_DEST, \"images\")\n",
    "LABEL_FILE = os.path.join(CSV_DEST, \"Data_Entry_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please make sure to download\n",
      "https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\n",
      "Data already exists\n",
      "CPU times: user 468 ms, sys: 136 ms, total: 604 ms\n",
      "Wall time: 604 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Download data\n",
    "print(\"Please make sure to download\")\n",
    "print(\"https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\")\n",
    "download_data_chextxray(CSV_DEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep\n",
    "https://github.com/apache/incubator-mxnet/issues/1480\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2682</td>\n",
       "      <td>2749</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>Cardiomegaly|Emphysema</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2894</td>\n",
       "      <td>2729</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>Cardiomegaly|Effusion</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2582</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
       "0  00000001_000.png            Cardiomegaly            0           1   \n",
       "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
       "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
       "3  00000002_000.png              No Finding            0           2   \n",
       "4  00000003_000.png                  Hernia            0           3   \n",
       "\n",
       "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0           58              M            PA                 2682     2749   \n",
       "1           58              M            PA                 2894     2729   \n",
       "2           58              M            PA                 2500     2048   \n",
       "3           81              M            PA                 2500     2048   \n",
       "4           81              F            PA                 2582     2991   \n",
       "\n",
       "   OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
       "0                        0.143  0.143          NaN  \n",
       "1                        0.143  0.143          NaN  \n",
       "2                        0.168  0.168          NaN  \n",
       "3                        0.171  0.171          NaN  \n",
       "4                        0.143  0.143          NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(LABEL_FILE)\n",
    "df.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df, img_dir, patient_ids):\n",
    "    # Split labels on unfiltered data\n",
    "    df_label = df['Finding Labels'].str.split(\n",
    "        '|', expand=False).str.join(sep='*').str.get_dummies(sep='*')\n",
    "\n",
    "    # Filter by patient-ids (both)\n",
    "    df_label['Patient ID'] = df['Patient ID']\n",
    "    df_label = df_label[df_label['Patient ID'].isin(patient_ids)]\n",
    "    df = df[df['Patient ID'].isin(patient_ids)]\n",
    "    # Remove unncessary columns\n",
    "    df_label.drop(['Patient ID','No Finding'], axis=1, inplace=True)  \n",
    "\n",
    "    # List of images (full-path)\n",
    "    img_locs =  df['Image Index'].map(lambda im: os.path.join(img_dir, im)).values\n",
    "    # One-hot encoded labels (float32 for BCE loss)\n",
    "    df_label['Image_path'] = img_locs\n",
    "    return df_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:21563 valid:3080 test:6162\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set, test_set = get_train_valid_test_split(TOT_PATIENT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XrayData(Dataset):\n",
    "    def __init__(self, img_dir, lbl_file, patient_ids, transform=None):\n",
    "        \n",
    "        self.img_locs, self.labels = get_imgloc_labels(img_dir, lbl_file, patient_ids)\n",
    "        self.transform = transform\n",
    "        print(\"Loaded {} labels and {} images\".format(len(self.labels), len(self.img_locs)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        im_file = self.img_locs[idx]\n",
    "        im_rgb = Image.open(im_file)\n",
    "        label = self.labels[idx]\n",
    "        im_rgb = mx.nd.array(im_rgb)\n",
    "        if self.transform is not None:\n",
    "            im_rgb = self.transform(im_rgb)\n",
    "\n",
    "        return im_rgb, mx.nd.array(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_augmentation_dataset(img_dir, lbl_file, patient_ids, normalize):\n",
    "    dataset = XrayData(img_dir, lbl_file, patient_ids,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize(WIDTH),\n",
    "                           transforms.ToTensor(),  \n",
    "                           transforms.Normalize(IMAGENET_RGB_MEAN, IMAGENET_RGB_SD)]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 87306 labels and 87306 images\n"
     ]
    }
   ],
   "source": [
    "# Dataset for training\n",
    "train_dataset = XrayData(img_dir=IMAGE_FOLDER,\n",
    "                         lbl_file=LABEL_FILE,\n",
    "                         patient_ids=train_set,\n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.RandomResizedCrop(size=WIDTH),\n",
    "                             transforms.RandomFlipLeftRight(),\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize(IMAGENET_RGB_MEAN, IMAGENET_RGB_SD)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7616 labels and 7616 images\n",
      "Loaded 17198 labels and 17198 images\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = no_augmentation_dataset(IMAGE_FOLDER, LABEL_FILE, valid_set, transforms.Normalize(IMAGENET_RGB_MEAN, IMAGENET_RGB_SD))\n",
    "test_dataset = no_augmentation_dataset(IMAGE_FOLDER, LABEL_FILE, test_set, transforms.Normalize(IMAGENET_RGB_MEAN, IMAGENET_RGB_SD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCHSIZE,\n",
    "                          shuffle=True, num_workers=CPU_COUNT, last_batch='discard')\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCHSIZE,\n",
    "                          shuffle=False, num_workers=CPU_COUNT, last_batch='discard')\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCHSIZE,\n",
    "                         shuffle=False, num_workers=CPU_COUNT, last_batch='discard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = [mx.gpu(i) for i in range(GPU_COUNT)]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = mx.gluon.model_zoo.vision.densenet121(pretrained=True, ctx=ctx)\n",
    "with net.name_scope():\n",
    "    net.output = mx.gluon.nn.Dense(CLASSES)\n",
    "net.output.initialize(ctx=ctx)\n",
    "net.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': LR})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_entropy = gluon.loss.SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = gluon.nn.Activation('sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = 0\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data_split = gluon.utils.split_and_load(data, ctx)\n",
    "        label_split = gluon.utils.split_and_load(label, ctx)\n",
    "        outputs = [(sig(net(X)),Y) for X, Y in zip(data_split, label_split)]\n",
    "        for output, label in outputs:\n",
    "            acc += float((label.asnumpy() == np.round(output.asnumpy())).sum()) / CLASSES / output.shape[0]\n",
    "    data_split = gluon.utils.split_and_load(data, [mx.cpu()])\n",
    "    label_split = gluon.utils.split_and_load(label, [mx.cpu()])\n",
    "    return acc/i/len(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: Sigmoid Binary Cross Entropy Loss: 0.8487\n",
      "Batch 200: Sigmoid Binary Cross Entropy Loss: 0.7617\n",
      "Batch 300: Sigmoid Binary Cross Entropy Loss: 0.7285\n",
      "Epoch 0, 0.985660 test_accuracy after 126.50 seconds\n",
      "Batch 100: Sigmoid Binary Cross Entropy Loss: 0.6509\n",
      "Batch 200: Sigmoid Binary Cross Entropy Loss: 0.6428\n",
      "Batch 300: Sigmoid Binary Cross Entropy Loss: 0.6409\n",
      "Epoch 1, 0.985680 test_accuracy after 115.59 seconds\n",
      "Batch 100: Sigmoid Binary Cross Entropy Loss: 0.6228\n",
      "Batch 200: Sigmoid Binary Cross Entropy Loss: 0.6241\n",
      "Batch 300: Sigmoid Binary Cross Entropy Loss: 0.6262\n",
      "Epoch 2, 0.985959 test_accuracy after 114.67 seconds\n",
      "Batch 100: Sigmoid Binary Cross Entropy Loss: 0.6203\n",
      "Batch 200: Sigmoid Binary Cross Entropy Loss: 0.6182\n",
      "Batch 300: Sigmoid Binary Cross Entropy Loss: 0.6200\n",
      "Epoch 3, 0.985511 test_accuracy after 114.19 seconds\n",
      "Batch 100: Sigmoid Binary Cross Entropy Loss: 0.6160\n",
      "Batch 200: Sigmoid Binary Cross Entropy Loss: 0.6134\n",
      "Batch 300: Sigmoid Binary Cross Entropy Loss: 0.6141\n",
      "Epoch 4, 0.985043 test_accuracy after 113.93 seconds\n",
      "CPU times: user 28min 10s, sys: 11min 41s, total: 39min 52s\n",
      "Wall time: 9min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_batch = 100\n",
    "n_batches = len(train_loader)\n",
    "for e in range(EPOCHS):\n",
    "    tick = time()\n",
    "    loss = 0\n",
    "    for i, (data, label) in enumerate(train_loader):        \n",
    "        data_split = gluon.utils.split_and_load(data, ctx)\n",
    "        label_split = gluon.utils.split_and_load(label, ctx)  \n",
    "        \n",
    "        # Printing the loss here to allow data to be loaded asynchronously on the GPU\n",
    "        if (i > 0):\n",
    "            loss += sum(losses).mean().asscalar()\n",
    "        if (i%n_batch == 0 and i > 0):\n",
    "            print('Batch {0}: Sigmoid Binary Cross Entropy Loss: {1:.4f}'.format(i,loss/i))            \n",
    "            \n",
    "        with autograd.record():\n",
    "            losses = [binary_cross_entropy(net(X), Y) for X, Y in zip(data_split, label_split)]\n",
    "        for l in losses:\n",
    "            l.backward()\n",
    "        trainer.step(data.shape[0]) \n",
    "    test_accuracy = evaluate_accuracy(valid_loader, net)\n",
    "    print('Epoch {0}, {1:.6f} test_accuracy after {2:.2f} seconds'.format(e, test_accuracy, time()-tick))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 s, sys: 11.7 s, total: 29.7 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = np.zeros((0, CLASSES))\n",
    "labels = np.zeros((0, CLASSES))\n",
    "for (data, label) in (test_loader):        \n",
    "    data_split = gluon.utils.split_and_load(data, ctx)\n",
    "    label_split = gluon.utils.split_and_load(label, ctx)  \n",
    "    outputs = [sig(net(X)) for X in data_split]\n",
    "    predictions = np.concatenate([predictions, np.concatenate([output.asnumpy() for output in outputs])])\n",
    "    labels = np.concatenate([labels, np.concatenate([label.asnumpy() for label in label_split])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full AUC [0.80908769611039455, 0.87312807723919572, 0.80886388318579316, 0.89052516742549404, 0.88370025678487119, 0.91963502792784457, 0.73497915084589172, 0.88040226735777372, 0.62529163818779587, 0.84906419830714797, 0.73238393223680975, 0.79835446517331077, 0.75521449476824731, 0.89192137073324274]\n",
      "Validation AUC: 0.8180\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation AUC: {0:.4f}\".format(compute_roc_auc(labels, predictions, CLASSES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data (Pure Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_X = mx.nd.ones((tot_num, 3, 224, 224), dtype=np.float32)\n",
    "fake_y = mx.nd.ones((tot_num, CLASSES), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_synth = ArrayDataset(fake_X, fake_y)\n",
    "train_dataloader_synth = DataLoader(train_dataset_synth, BATCHSIZE, shuffle=False, num_workers=CPU_COUNT, last_batch='discard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50: Sigmoid Binary Cross Entropy Loss: 0.6370\n",
      "Batch 100: Sigmoid Binary Cross Entropy Loss: 0.6420\n",
      "Batch 150: Sigmoid Binary Cross Entropy Loss: 0.6380\n",
      "Batch 200: Sigmoid Binary Cross Entropy Loss: 0.6385\n",
      "Batch 250: Sigmoid Binary Cross Entropy Loss: 0.6373\n",
      "Batch 300: Sigmoid Binary Cross Entropy Loss: 0.6377\n",
      "Epoch 0, 106.00 seconds, loss 0.6637\n",
      "Batch 50: Sigmoid Binary Cross Entropy Loss: 0.6239\n",
      "Batch 100: Sigmoid Binary Cross Entropy Loss: 0.6236\n",
      "Batch 150: Sigmoid Binary Cross Entropy Loss: 0.6264\n",
      "Batch 200: Sigmoid Binary Cross Entropy Loss: 0.6254\n",
      "Batch 250: Sigmoid Binary Cross Entropy Loss: 0.6258\n",
      "Batch 300: Sigmoid Binary Cross Entropy Loss: 0.6248\n",
      "Epoch 1, 105.55 seconds, loss 0.6105\n",
      "Batch 50: Sigmoid Binary Cross Entropy Loss: 0.6228\n",
      "Batch 100: Sigmoid Binary Cross Entropy Loss: 0.6206\n",
      "Batch 150: Sigmoid Binary Cross Entropy Loss: 0.6210\n",
      "Batch 200: Sigmoid Binary Cross Entropy Loss: 0.6187\n",
      "Batch 250: Sigmoid Binary Cross Entropy Loss: 0.6204\n",
      "Batch 300: Sigmoid Binary Cross Entropy Loss: 0.6196\n",
      "Epoch 2, 106.46 seconds, loss 0.5889\n",
      "Batch 50: Sigmoid Binary Cross Entropy Loss: 0.6152\n",
      "Batch 100: Sigmoid Binary Cross Entropy Loss: 0.6116\n",
      "Batch 150: Sigmoid Binary Cross Entropy Loss: 0.6103\n",
      "Batch 200: Sigmoid Binary Cross Entropy Loss: 0.6105\n",
      "Batch 250: Sigmoid Binary Cross Entropy Loss: 0.6121\n",
      "Batch 300: Sigmoid Binary Cross Entropy Loss: 0.6116\n",
      "Epoch 3, 106.45 seconds, loss 0.6165\n",
      "Batch 50: Sigmoid Binary Cross Entropy Loss: 0.6031\n",
      "Batch 100: Sigmoid Binary Cross Entropy Loss: 0.6039\n",
      "Batch 150: Sigmoid Binary Cross Entropy Loss: 0.6055\n",
      "Batch 200: Sigmoid Binary Cross Entropy Loss: 0.6052\n",
      "Batch 250: Sigmoid Binary Cross Entropy Loss: 0.6067\n",
      "Batch 300: Sigmoid Binary Cross Entropy Loss: 0.6068\n",
      "Epoch 4, 106.50 seconds, loss 0.6135\n",
      "CPU times: user 26min 31s, sys: 11min 43s, total: 38min 15s\n",
      "Wall time: 8min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_batch = 50\n",
    "for e in range(EPOCHS):\n",
    "    tick = time()\n",
    "    loss = 0\n",
    "    for i, (data, label) in enumerate(train_loader):        \n",
    "        data_split = gluon.utils.split_and_load(data, ctx)\n",
    "        label_split = gluon.utils.split_and_load(label, ctx)  \n",
    "        \n",
    "        # Printing the loss here to allow data to be loaded asynchronously on the GPU\n",
    "        if (i > 0):\n",
    "            loss += sum(losses).mean().asscalar()\n",
    "        if (i%n_batch == 0 and i > 0):\n",
    "            print('Batch {0}: Sigmoid Binary Cross Entropy Loss: {1:.4f}'.format(i,loss/i))            \n",
    "            \n",
    "        with autograd.record():\n",
    "            losses = [binary_cross_entropy(net(X), Y) for X, Y in zip(data_split, label_split)]\n",
    "        for l in losses:\n",
    "            l.backward()\n",
    "        trainer.step(data.shape[0]) \n",
    "\n",
    "    print('Epoch {0}, {1:.2f} seconds, loss {2:.4f}'.format(e, time()-tick, sum(losses).mean().asscalar()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
